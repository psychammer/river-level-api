# evaluate_models.py
"""
Offline Evaluation Script for Water Level Forecasting Models.

This script loads the pre-trained 'full' and 'ablated' models and evaluates
their performance across the entire 'imputed_features.npy' dataset.

It calculates the average MAE, RMSE, and NSE for each forecast horizon
for the sole purpose of generating the final metrics to be hardcoded into the API.

To Run:
1. Make sure you have the required libraries (torch, numpy, etc.).
2. Place this file in the root of your project directory.
3. Run `python evaluate_models.py` from your terminal.
"""
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from torch_geometric.nn import GCNConv, GATConv
import math
import os

# --- Configuration ---
# All paths point to the /models directory
import torch
import torch.nn as nn
import numpy as np
from tqdm import tqdm
from torch_geometric.nn import GCNConv, GATConv
import math
import os

# --- Configuration ---
class EvalConfig:
    MODEL_DIR = './models/'
    FEATURES_FILE = os.path.join(MODEL_DIR, 'imputed_features.npy')
    ADJ_FILE = os.path.join(MODEL_DIR, 'adjacency_matrix.npy')
    SCALER_MEAN_FILE = os.path.join(MODEL_DIR, 'scaler_mean.npy')
    SCALER_STD_FILE = os.path.join(MODEL_DIR, 'scaler_std.npy')

    # Model Paths
    FULL_MODEL_PATH = os.path.join(MODEL_DIR, 'full_forecaster_model.pth')
    ABLATED_MODEL_PATH = os.path.join(MODEL_DIR, 'ablated_forecaster_model.pth')
    ENCODER_PATH = os.path.join(MODEL_DIR, 'stgae_encoder.pth')

    # Data & Model Specs (must match training)
    NUM_STATIONS = 5
    NUM_FEATURES = 10
    TARGET_FEATURE_IDX = 0
    LOOKBACK_WINDOW = 24 * 3
    FORECAST_HORIZONS = [1, 3, 6, 12, 24, 48]
    # NEW: Added station order for clear reporting
    STATION_ORDER = ['MONTALBAN', 'NANGKA', 'SAN MATEO', 'STO NINO', 'TUMANA']
    
    # Hyperparameters (must match training)
    STGAE_GCN_HIDDEN = 64
    STGAE_GRU_HIDDEN_FACTOR = 64
    GAT_IN_FEATURES_FULL = STGAE_GCN_HIDDEN
    GAT_IN_FEATURES_ABLATED = NUM_FEATURES
    GAT_HIDDEN_DIM = 32
    GAT_HEADS = 4
    TRANSFORMER_D_MODEL = GAT_HIDDEN_DIM * GAT_HEADS
    TRANSFORMER_HEADS = 4
    TRANSFORMER_LAYERS = 3
    TRANSFORMER_FF_DIM = 256
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

config = EvalConfig()
# --- Model Definitions (Copied from training scripts) ---
# These are required for PyTorch to load the saved models correctly.

class FrozenSTGAEEncoder(nn.Module):
    def __init__(self, in_features, gcn_hidden, gru_hidden, encoder_weights_path):
        super().__init__()
        self.encoder_gcn = GCNConv(in_features, gcn_hidden)
        self.encoder_gru = nn.GRU(gcn_hidden * config.NUM_STATIONS, gru_hidden, batch_first=True)
        self.activation = nn.Tanh()
        self.load_encoder_weights(encoder_weights_path)
        for param in self.parameters(): param.requires_grad = False
    
    def load_encoder_weights(self, weights_path):
        encoder_state = torch.load(weights_path, map_location=config.DEVICE)
        self.encoder_gcn.load_state_dict(encoder_state['encoder_gcn'])
        self.encoder_gru.load_state_dict(encoder_state['encoder_gru'])
    
    def forward(self, x, edge_index):
        batch_size, seq_len, num_nodes, _ = x.shape
        gcn_outputs = []
        for t in range(seq_len):
            xt = x[:, t, :, :].reshape(-1, x.size(3))
            edge_index_batched = edge_index.clone()
            for i in range(1, batch_size):
                edge_index_batched = torch.cat([edge_index_batched, edge_index + i * num_nodes], dim=1)
            gcn_out = self.activation(self.encoder_gcn(xt, edge_index_batched)).reshape(batch_size, num_nodes, -1)
            gcn_outputs.append(gcn_out)
        gcn_features = torch.stack(gcn_outputs, dim=1)
        gru_in = gcn_features.reshape(batch_size, seq_len, -1)
        _, temporal_encoding = self.encoder_gru(gru_in)
        return gcn_features, temporal_encoding.squeeze(0)

class GraphAttentionLayer(nn.Module):
    def __init__(self, in_features, hidden_dim, heads):
        super().__init__()
        self.gat = GATConv(in_features, hidden_dim, heads=heads, concat=True, dropout=0.1)
        self.norm = nn.LayerNorm(hidden_dim * heads)
    def forward(self, x, edge_index):
        return self.norm(self.gat(x, edge_index))

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)
    def forward(self, x): return x + self.pe[:x.size(0), :]

class TemporalTransformer(nn.Module):
    def __init__(self, d_model, n_heads, n_layers, ff_dim, max_seq_len):
        super().__init__()
        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=ff_dim, dropout=0.1, batch_first=True)
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
    def forward(self, x):
        x = x.transpose(0, 1)
        x = self.pos_encoding(x)
        x = x.transpose(0, 1)
        return self.transformer(x)

class MultiHorizonForecastHead(nn.Module):
    def __init__(self, input_dim, horizons, num_stations):
        super().__init__()
        self.forecast_heads = nn.ModuleDict({
            f'horizon_{h}': nn.Sequential(nn.Linear(input_dim, input_dim // 2), nn.ReLU(), nn.Dropout(0.1), nn.Linear(input_dim // 2, num_stations))
            for h in horizons
        })
    def forward(self, x):
        return {h: self.forecast_heads[f'horizon_{h}'](x) for h in config.FORECAST_HORIZONS}

class STGAEGATTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.stgae_encoder = FrozenSTGAEEncoder(config.NUM_FEATURES, config.STGAE_GCN_HIDDEN, config.STGAE_GRU_HIDDEN_FACTOR * config.NUM_STATIONS, config.ENCODER_PATH)
        self.gat = GraphAttentionLayer(config.GAT_IN_FEATURES_FULL, config.GAT_HIDDEN_DIM, config.GAT_HEADS)
        self.transformer = TemporalTransformer(config.TRANSFORMER_D_MODEL, config.TRANSFORMER_HEADS, config.TRANSFORMER_LAYERS, config.TRANSFORMER_FF_DIM, config.LOOKBACK_WINDOW)
        self.forecast_head = MultiHorizonForecastHead(config.TRANSFORMER_D_MODEL, config.FORECAST_HORIZONS, config.NUM_STATIONS)
    def forward(self, x, edge_index):
        batch_size, seq_len, num_nodes, _ = x.shape
        with torch.no_grad(): gcn_features, _ = self.stgae_encoder(x, edge_index)
        gcn_flat = gcn_features.reshape(-1, gcn_features.size(-1))
        edge_index_batched = edge_index.clone()
        for i in range(1, batch_size * seq_len): edge_index_batched = torch.cat([edge_index_batched, edge_index + i * num_nodes], dim=1)
        gat_out = self.gat(gcn_flat, edge_index_batched)
        gat_features = gat_out.reshape(batch_size, seq_len, num_nodes, -1)
        transformer_input = gat_features.mean(dim=2)
        transformer_out = self.transformer(transformer_input)
        return self.forecast_head(transformer_out[:, -1, :])

class AblatedGATTransformer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.gat = GraphAttentionLayer(config.GAT_IN_FEATURES_ABLATED, config.GAT_HIDDEN_DIM, config.GAT_HEADS)
        self.transformer = TemporalTransformer(config.TRANSFORMER_D_MODEL, config.TRANSFORMER_HEADS, config.TRANSFORMER_LAYERS, config.TRANSFORMER_FF_DIM, config.LOOKBACK_WINDOW)
        self.forecast_head = MultiHorizonForecastHead(config.TRANSFORMER_D_MODEL, config.FORECAST_HORIZONS, config.NUM_STATIONS)
    def forward(self, x, edge_index):
        batch_size, seq_len, num_nodes, num_features = x.shape
        x_flat = x.reshape(-1, num_features)
        edge_index_batched = edge_index.clone()
        for i in range(1, batch_size * seq_len): edge_index_batched = torch.cat([edge_index_batched, edge_index + i * num_nodes], dim=1)
        gat_out = self.gat(x_flat, edge_index_batched)
        gat_features = gat_out.reshape(batch_size, seq_len, num_nodes, -1)
        transformer_input = gat_features.mean(dim=2)
        transformer_out = self.transformer(transformer_input)
        return self.forecast_head(transformer_out[:, -1, :])


# --- Metrics Calculation ---
class PerformanceMetrics:
    @staticmethod
    def calculate_mae(y_true, y_pred): return np.nanmean(np.abs(y_pred - y_true))
    @staticmethod
    def calculate_rmse(y_true, y_pred): return np.sqrt(np.nanmean((y_pred - y_true)**2))
    @staticmethod
    def calculate_nse(y_true, y_pred):
        mask = ~np.isnan(y_true)
        if not np.any(mask): return np.nan
        y_true_masked = y_true[mask]
        y_pred_masked = y_pred[mask]
        numerator = np.sum((y_true_masked - y_pred_masked)**2)
        denominator = np.sum((y_true_masked - np.mean(y_true_masked))**2)
        return 1 - (numerator / denominator) if denominator != 0 else np.nan

# --- Main Evaluation Function ---
def run_evaluation():
    print(f"--- Starting Offline Model Evaluation (Per-Station) on {config.DEVICE} ---")
    
    # 1. Load data, scalers, and adjacency matrix
    print("Loading data and supporting files...")
    features = np.load(config.FEATURES_FILE)
    adj_matrix = np.load(config.ADJ_FILE)
    mean = np.load(config.SCALER_MEAN_FILE)
    std = np.load(config.SCALER_STD_FILE)
    edge_index = torch.tensor(np.where(adj_matrix > 0), dtype=torch.long).to(config.DEVICE)
    
    # 2. Load trained models
    full_model = STGAEGATTransformer(config).to(config.DEVICE)
    full_model.load_state_dict(torch.load(config.FULL_MODEL_PATH, map_location=config.DEVICE))
    full_model.eval()
    print("âœ… Full model loaded successfully.")
    
    ablated_model = AblatedGATTransformer(config).to(config.DEVICE)
    ablated_model.load_state_dict(torch.load(config.ABLATED_MODEL_PATH, map_location=config.DEVICE))
    ablated_model.eval()
    print("âœ… Ablated model loaded successfully.")

    # 3. Scale the data
    scaled_features = (features - mean) / (std + 1e-8)
    
    # 4. Perform sliding window evaluation
    print(f"\nRunning evaluation with a sliding window across {len(features)} timesteps...")
    
    # MODIFIED: New data structure to hold raw predictions and actuals per station
    all_results = {
        model: {
            station: {h: {'preds': [], 'actuals': []} for h in config.FORECAST_HORIZONS}
            for station in config.STATION_ORDER
        }
        for model in ['full', 'ablated']
    }

    max_horizon = max(config.FORECAST_HORIZONS)
    num_predictions = len(scaled_features) - config.LOOKBACK_WINDOW - max_horizon
    
    for i in tqdm(range(num_predictions), desc="Evaluating"):
        input_start = i
        input_end = i + config.LOOKBACK_WINDOW
        input_tensor = torch.tensor(scaled_features[input_start:input_end], dtype=torch.float32).unsqueeze(0).to(config.DEVICE)
        
        with torch.no_grad():
            full_preds_scaled = full_model(torch.nan_to_num(input_tensor), edge_index)
            ablated_preds_scaled = ablated_model(torch.nan_to_num(input_tensor), edge_index)

        for h in config.FORECAST_HORIZONS:
            truth_idx = input_end + h - 1
            actual_scaled = scaled_features[truth_idx, :, config.TARGET_FEATURE_IDX]
            
            wl_mean = mean[config.TARGET_FEATURE_IDX]
            wl_std = std[config.TARGET_FEATURE_IDX]
            actual_values = (actual_scaled * wl_std) + wl_mean
            
            full_pred_values = (full_preds_scaled[h].squeeze().cpu().numpy() * wl_std) + wl_mean
            ablated_pred_values = (ablated_preds_scaled[h].squeeze().cpu().numpy() * wl_std) + wl_mean
            
            # MODIFIED: Store results for each station separately
            for s_idx, station_name in enumerate(config.STATION_ORDER):
                all_results['full'][station_name][h]['preds'].append(full_pred_values[s_idx])
                all_results['full'][station_name][h]['actuals'].append(actual_values[s_idx])
                
                all_results['ablated'][station_name][h]['preds'].append(ablated_pred_values[s_idx])
                all_results['ablated'][station_name][h]['actuals'].append(actual_values[s_idx])

    # 5. Calculate final metrics from the collected results and print
    print("\n\n" + "="*80)
    print("--- FINAL AVERAGED PERFORMANCE METRICS (PER-STATION) ---")
    print("="*80)
    
    for model_name, station_data in all_results.items():
        print(f"\n\n--- MODEL: {model_name.upper()} ---")
        for station_name, horizon_data in station_data.items():
            print(f"\nðŸ“Š Station: {station_name}")
            print("   Horizon | Avg. MAE | Avg. RMSE | Avg. NSE")
            print("   " + "-"*45)
            for h in config.FORECAST_HORIZONS:
                preds = np.array(horizon_data[h]['preds'])
                actuals = np.array(horizon_data[h]['actuals'])
                
                avg_mae = PerformanceMetrics.calculate_mae(actuals, preds)
                avg_rmse = PerformanceMetrics.calculate_rmse(actuals, preds)
                avg_nse = PerformanceMetrics.calculate_nse(actuals, preds)
                
                print(f"   {h:<7}h | {avg_mae:8.4f} | {avg_rmse:9.4f} | {avg_nse:8.4f}")
    
    print("\n" + "="*80)
    print("Evaluation complete. You can now use these detailed values.")

if __name__ == "__main__":
    run_evaluation()